{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import tensorflow as tf  # for data preprocessing\n",
    "import nibabel as nib\n",
    "from scipy import ndimage\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "# tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CT scans with normal lung tissue: 4\n"
     ]
    }
   ],
   "source": [
    "def read_nifti_file(filepath):\n",
    "    \"\"\"Read and load volume\"\"\"\n",
    "    # Read file\n",
    "    try:\n",
    "        scan = nib.load(filepath)\n",
    "        # Get raw data\n",
    "        scan = scan.get_fdata()\n",
    "        return scan\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "scan_paths = [\n",
    "    os.path.join(os.getcwd(), \"../image_slicing/sample\", x)\n",
    "    for x in os.listdir(\"../image_slicing/sample\")\n",
    "]\n",
    "\n",
    "print(\"CT scans with normal lung tissue: \" + str(len(scan_paths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3}\n"
     ]
    }
   ],
   "source": [
    "# Scan masks to sort between positive and negative samples\n",
    "mask_scans_list = [read_nifti_file(path + '/MASK.nii.gz') for path in scan_paths]\n",
    "ok_idx_mask = [i for i, x in enumerate(mask_scans_list) if x is not None]\n",
    "ct_scans_list = [read_nifti_file(path + '/CT.nii.gz') for path in scan_paths]\n",
    "ok_idx_ct = [i for i, x in enumerate(ct_scans_list) if x is not None]\n",
    "pet_scans_list = [read_nifti_file(path + '/PET.nii.gz') for path in scan_paths]\n",
    "ok_idx_pet = [i for i, x in enumerate(pet_scans_list) if x is not None]\n",
    "\n",
    "# Filter for corrupted files\n",
    "ok_indices = set(ok_idx_mask) & set(ok_idx_ct) & set(ok_idx_pet)\n",
    "print(ok_indices)\n",
    "\n",
    "ct_scans_list = [ct_scans_list[i] for i in ok_indices]\n",
    "mask_scans_list = [mask_scans_list[i] for i in ok_indices]\n",
    "pet_scans_list = [pet_scans_list[i] for i in ok_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find positive and negative samples: if the mask has any pixel > 0 it is a positive sample\n",
    "pos_indices = np.where([np.max(mask) > 0 for mask in mask_scans_list])[0]\n",
    "del mask_scans_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(volume):\n",
    "    \"\"\"Normalize the volume\"\"\"\n",
    "    min = -1000\n",
    "    max = 400\n",
    "    volume[volume < min] = min\n",
    "    volume[volume > max] = max\n",
    "    volume = (volume - min) / (max - min)\n",
    "    volume = volume.astype(\"float32\")\n",
    "    return volume\n",
    "\n",
    "\n",
    "def resize_volume(img):\n",
    "    \"\"\"Resize across z-axis\"\"\"\n",
    "    # Set the desired depth\n",
    "    desired_depth = 250\n",
    "    desired_width = 350\n",
    "    desired_height = 350\n",
    "    # Get current depth\n",
    "    current_depth = img.shape[-1]\n",
    "    current_width = img.shape[0]\n",
    "    current_height = img.shape[1]\n",
    "    # Compute depth factor\n",
    "    depth = current_depth / desired_depth\n",
    "    width = current_width / desired_width\n",
    "    height = current_height / desired_height\n",
    "    depth_factor = 1 / depth\n",
    "    width_factor = 1 / width\n",
    "    height_factor = 1 / height\n",
    "    # Rotate\n",
    "    #img = ndimage.rotate(img, 90, reshape=False)\n",
    "    # Resize across z-axis\n",
    "    img = ndimage.zoom(img, (width_factor, height_factor, depth_factor), order=1)\n",
    "    img = img.astype(np.float16)\n",
    "    return img\n",
    "\n",
    "def process_scan(scan):\n",
    "    \"\"\"Read and resize volume\"\"\"\n",
    "    # Normalize\n",
    "    volume = normalize(scan)\n",
    "    # Resize width, height and depth\n",
    "    volume = resize_volume(volume)\n",
    "    return volume\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask_scans_processed = np.array([process_scan(scan) for scan in mask_scans_list])\n",
    "ct_scans_processed = np.array([process_scan(scan) for scan in ct_scans_list])\n",
    "combined_scans = 0.02 * ct_scans_processed\n",
    "del ct_scans_processed\n",
    "del ct_scans_list\n",
    "\n",
    "pet_scans_processed = np.array([process_scan(scan) for scan in pet_scans_list])\n",
    "combined_scans += 0.98 * pet_scans_processed\n",
    "del pet_scans_processed\n",
    "del pet_scans_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = combined_scans[..., np.newaxis]  # Add channel dimension\n",
    "y = np.array([1 if i in pos_indices else 0 for i in range(len(combined_scans))])  # Assuming pos_indices identifies positive samples\n",
    "del combined_scans\n",
    "\n",
    "# Split data (consider using sklearn's train_test_split)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/domen/mypython/lib/python3.9/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Conv3D(16, kernel_size=(3, 3, 3), activation='relu', input_shape=(350, 350, 250, 1)),  # Adjusted input shape\n",
    "    MaxPooling3D(pool_size=(2, 2, 2)),\n",
    "    Conv3D(32, kernel_size=(3, 3, 3), activation='relu'),\n",
    "    MaxPooling3D(pool_size=(2, 2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AFTER:\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "\n",
    "# Checkpoint directory\n",
    "checkpoint_dir = \"checkpoints/\"\n",
    "\n",
    "# Create directory if it does not exist\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "checkpoint_path = \"checkpoints/model_{epoch:02d}-{val_loss:.2f}.keras\"\n",
    "\n",
    "# Create a ModelCheckpoint callback\n",
    "checkpoint = ModelCheckpoint(filepath=checkpoint_path,\n",
    "                             save_best_only=True,  # Save only the best model\n",
    "                             monitor='val_loss',   # Monitor the validation loss\n",
    "                             mode='min',           # Minimize the validation loss\n",
    "                             verbose=1)            # Print out when a model is being saved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass this callback to the `fit` method\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=1,\n",
    "    verbose=1,\n",
    "    batch_size=2,\n",
    "    callbacks=[checkpoint]  # Include the callback here\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
