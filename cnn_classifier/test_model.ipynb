{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Images\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Reading\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import tensorflow as tf  # for data preprocessing\n",
    "import nibabel as nib\n",
    "from scipy import ndimage\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "import gc\n",
    "from tensorflow.keras import Input, Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense\n",
    "\n",
    "# AFTER:\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "\n",
    "\n",
    "# tf.enable_eager_execution()\n",
    "\n",
    "\n",
    "# %%\n",
    "def read_nifti_file(filepath):\n",
    "    \"\"\"Read and load volume\"\"\"\n",
    "    # Read file\n",
    "    try:\n",
    "        scan = nib.load(filepath)\n",
    "        # Get raw data\n",
    "        scan = scan.get_fdata()\n",
    "        return scan\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "\n",
    "# %%\n",
    "def normalize(volume):\n",
    "    \"\"\"Normalize the volume\"\"\"\n",
    "    min = -1000\n",
    "    max = 400\n",
    "    volume[volume < min] = min\n",
    "    volume[volume > max] = max\n",
    "    volume = (volume - min) / (max - min)\n",
    "    volume = volume.astype(\"float32\")\n",
    "    return volume\n",
    "\n",
    "\n",
    "def resize_volume(img):\n",
    "    \"\"\"Resize across z-axis\"\"\"\n",
    "    # Set the desired depth\n",
    "    desired_depth = 250\n",
    "    desired_width = 350\n",
    "    desired_height = 350\n",
    "    # Get current depth\n",
    "    current_depth = img.shape[-1]\n",
    "    current_width = img.shape[0]\n",
    "    current_height = img.shape[1]\n",
    "    # Compute depth factor\n",
    "    depth = current_depth / desired_depth\n",
    "    width = current_width / desired_width\n",
    "    height = current_height / desired_height\n",
    "    depth_factor = 1 / depth\n",
    "    width_factor = 1 / width\n",
    "    height_factor = 1 / height\n",
    "    # Rotate\n",
    "    # img = ndimage.rotate(img, 90, reshape=False)\n",
    "    # Resize across z-axis\n",
    "    img = ndimage.zoom(img, (width_factor, height_factor, depth_factor), order=1)\n",
    "    img = img.astype(np.float16)\n",
    "    return img\n",
    "\n",
    "\n",
    "def process_scan(scan):\n",
    "    \"\"\"Read and resize volume\"\"\"\n",
    "    # Normalize\n",
    "    volume = normalize(scan)\n",
    "    # Resize width, height and depth\n",
    "    volume = resize_volume(volume)\n",
    "    return volume\n",
    "\n",
    "\n",
    "scan_paths_main = [\n",
    "    os.path.join(\"/d/hpc/projects/training/RIS/data/RIS\", x)\n",
    "    for x in os.listdir(\"/d/hpc/projects/training/RIS/data/RIS\")\n",
    "]\n",
    "\n",
    "print(\"all paths: \" + str(len(scan_paths_main)))\n",
    "scan_paths_main = sorted(scan_paths_main)\n",
    "\n",
    "scan_paths = scan_paths_main\n",
    "print(\"used paths: \" + str(len(scan_paths_main)))\n",
    "combined_scans_main = []\n",
    "pos_indices_main = []\n",
    "\n",
    "\n",
    "# %%\n",
    "# Scan masks to sort between positive and negative samples\n",
    "mask_scans_list = [read_nifti_file(path + \"/MASK.nii.gz\") for path in scan_paths]\n",
    "ok_idx_mask = [i for i, x in enumerate(mask_scans_list) if x is not None]\n",
    "ct_scans_list = [read_nifti_file(path + \"/CT.nii.gz\") for path in scan_paths]\n",
    "ok_idx_ct = [i for i, x in enumerate(ct_scans_list) if x is not None]\n",
    "pet_scans_list = [read_nifti_file(path + \"/PET.nii.gz\") for path in scan_paths]\n",
    "ok_idx_pet = [i for i, x in enumerate(pet_scans_list) if x is not None]\n",
    "\n",
    "# Filter for corrupted files\n",
    "ok_indices = set(ok_idx_mask) & set(ok_idx_ct) & set(ok_idx_pet)\n",
    "print(ok_indices)\n",
    "\n",
    "ct_scans_list = [ct_scans_list[i] for i in ok_indices]\n",
    "mask_scans_list = [mask_scans_list[i] for i in ok_indices]\n",
    "pet_scans_list = [pet_scans_list[i] for i in ok_indices]\n",
    "\n",
    "# %%\n",
    "# find positive and negative samples: if the mask has any pixel > 0 it is a positive sample\n",
    "pos_indices = np.where([np.max(mask) > 0 for mask in mask_scans_list])[0]\n",
    "print(pos_indices)\n",
    "pos_indices_main.extend(pos_indices)\n",
    "del mask_scans_list\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Processing\n",
    "\n",
    "# %%\n",
    "# mask_scans_processed = np.array([process_scan(scan) for scan in mask_scans_list])\n",
    "ct_scans_processed = np.array([process_scan(scan) for scan in ct_scans_list])\n",
    "combined_scans = 0.02 * ct_scans_processed\n",
    "del ct_scans_processed\n",
    "del ct_scans_list\n",
    "\n",
    "pet_scans_processed = np.array([process_scan(scan) for scan in pet_scans_list])\n",
    "combined_scans += 0.98 * pet_scans_processed\n",
    "del pet_scans_processed\n",
    "del pet_scans_list\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
